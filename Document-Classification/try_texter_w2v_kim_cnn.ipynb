{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%xmode plain\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from texter.utils import text_utils as tu\n",
    "from texter.utils import embedding_utils as eu\n",
    "from texter.models.keras.kim_cnn import kim_cnn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(15101993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludes = stopwords.words(\"english\")\n",
    "text_path = \"../../../citi/citi_rest/data/raw_data/\"\n",
    "text_labels = os.listdir(text_path)\n",
    "pretrained_path = \"../../data/others/GoogleNews-vectors-negative300-SLIM.bin\"\n",
    "pre_path = \"../../data/others/glove.6B.50d.txt\"\n",
    "data = tu.load_data(excludes, text_path, text_labels)\n",
    "\n",
    "texts = np.array([t for t in data.text])\n",
    "labels = np.array([l for l in data.label])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_valid=tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (8, 30) (3, 30)\n",
      "Shape of label train and validation tensor: (8, 3) (3, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "Y_train = to_categorical(y_train, 3)\n",
    "Y_val = to_categorical(y_test, 3)\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', Y_train.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 30, 300)      17400       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 30, 300, 1)   0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 1, 100)   90100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 27, 1, 100)   120100      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 26, 1, 100)   150100      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 300)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            903         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 378,603\n",
      "Trainable params: 378,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emblay = eu.word2vec_embedding_layer(pretrained_path, word_index=word_index)\n",
    "#emblay = eu.glove_embedding_layer(pre_path, word_index=word_index, EMBEDDING_DIM=50)\n",
    "sequence_length = X_train.shape[1]\n",
    "model = kim_cnn(sequence_length, 300, embedding_layer=emblay, n_class=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8 samples, validate on 3 samples\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.2122 - acc: 0.2500 - val_loss: 1.1602 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.1257 - acc: 0.8750 - val_loss: 1.1036 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0396 - acc: 1.0000 - val_loss: 1.0504 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.9557 - acc: 1.0000 - val_loss: 0.9973 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8881 - acc: 1.0000 - val_loss: 0.9438 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8364 - acc: 1.0000 - val_loss: 0.8914 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7344 - acc: 1.0000 - val_loss: 0.8425 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6660 - acc: 1.0000 - val_loss: 0.7976 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6036 - acc: 1.0000 - val_loss: 0.7564 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5509 - acc: 1.0000 - val_loss: 0.7213 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a674c2ac8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, Y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, Y_val),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
